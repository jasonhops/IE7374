{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df88e4f6-fd07-49ad-82c5-15e867d469c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                  title  \\\n",
      "0           0    No-Bake Nut Cookies   \n",
      "1           1  Jewell Ball'S Chicken   \n",
      "2           2            Creamy Corn   \n",
      "3           3          Chicken Funny   \n",
      "4           4   Reeses Cups(Candy)     \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
      "1  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
      "2  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
      "3  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
      "4  [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
      "\n",
      "                                          directions  \\\n",
      "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
      "1  [\"Place chipped beef on bottom of baking dish....   \n",
      "2  [\"In a slow cooker, combine all ingredients. C...   \n",
      "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
      "4  [\"Combine first four ingredients and press in ...   \n",
      "\n",
      "                                              link    source  \\\n",
      "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
      "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
      "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n",
      "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered   \n",
      "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered   \n",
      "\n",
      "                                                 NER  \n",
      "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
      "1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n",
      "2  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n",
      "3  [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n",
      "4  [\"peanut butter\", \"graham cracker crumbs\", \"bu...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"./RecipeNLG/RecipeNLG_dataset.csv\")\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac947b93-2a7b-460f-af6c-0ac887c39658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a 1% sample\n",
    "sample_1 = df.sample(frac=0.01, random_state=42)\n",
    "sample_1.to_csv(\"RecipeNLG/RecipeNLG_dataset_1pct.csv\", index=False)\n",
    "\n",
    "# Create a 5% sample\n",
    "sample_5 = df.sample(frac=0.05, random_state=42)\n",
    "sample_5.to_csv(\"RecipeNLG/RecipeNLG_dataset_5pct.csv\", index=False)\n",
    "\n",
    "print(\"Samples saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf3c19b-b7dc-4225-9e79-22bde42926fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     22311\n",
       "title          22311\n",
       "ingredients    22311\n",
       "directions     22311\n",
       "link           22311\n",
       "source         22311\n",
       "NER            22311\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47af9510-a15d-451c-acc7-2c7b7f3a0952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      int64\n",
       "title          object\n",
       "ingredients    object\n",
       "directions     object\n",
       "link           object\n",
       "source         object\n",
       "NER            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4769cab6-a02c-4a18-89cd-16fcbf9831cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Columns that look like stringified lists\n",
    "list_columns = [\"ingredients\", \"directions\", \"NER\"]\n",
    "\n",
    "for col in list_columns:\n",
    "    sample_1[col] = sample_1[col].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de89f6f6-4677-4479-886f-f2605314c3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015528    [flank steak, green onions, red wine, soy sauc...\n",
       "1608734    [rosemary, thyme, bay leaves, paprika, pepper,...\n",
       "778500            [carrots, butter, brown sugar, lemon rind]\n",
       "1334975    [Flour, Salt, Baking Powder, Sugar, Crisco, eg...\n",
       "116562                            [thin pretzels, margarine]\n",
       "Name: NER, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_1['NER'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84dbc2ec-ddc9-42e8-8053-33c34481632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER[0]: flank steak, green onions, red wine, soy sauce, salad oil, sesame seeds, brown sugar, grnd black pepper, grnd ginger, clove garlic, \n",
      "NER[1]: rosemary, thyme, bay leaves, paprika, pepper, red wine, chicken broth, button mushrooms, mushroom mix, carrots, onion, frozen green beans, black olives, handful grape tomatoes, chicken, stalks celery, water, \n",
      "NER[2]: carrots, butter, brown sugar, lemon rind, \n",
      "NER[3]: Flour, Salt, Baking Powder, Sugar, Crisco, egg, vinegar, Water, \n",
      "NER[4]: thin pretzels, margarine, \n"
     ]
    }
   ],
   "source": [
    "# Print NER items from first 5 rows (or any n)\n",
    "for i in range(5):\n",
    "    print(f\"NER[{i}]:\", end=\" \")\n",
    "    \n",
    "    for j in range(len(sample_1['NER'].iloc[i])):\n",
    "        print(sample_1['NER'].iloc[i][j], end=\", \")\n",
    "    \n",
    "    print()  # Newline after each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7dda1c1-644e-4a91-b683-7a6c84196e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing data\n",
    "sample_1 = sample_1.dropna(subset=[\"ingredients\", \"directions\"])\n",
    "\n",
    "# Optional: Keep only relevant columns\n",
    "sample_1 = sample_1[[\"ingredients\", \"directions\"]]\n",
    "\n",
    "# Convert each example into a prompt-response format\n",
    "def format_example(row):\n",
    "    return f\"\"\"### Ingredients:\\n{row['ingredients']}\\n\\n### Directions:\\n{row['directions']}\"\"\"\n",
    "\n",
    "# Apply formatting\n",
    "sample_1[\"formatted\"] = sample_1.apply(format_example, axis=1)\n",
    "\n",
    "# Save to text file (line-by-line for fine-tuning)\n",
    "with open(\"./RecipeNLG/RecipeNLG_dataset_gpt_in_1pct.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in sample_1[\"formatted\"]:\n",
    "        f.write(line + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6933e5-8d30-4b03-be81-6b6c0e47f366",
   "metadata": {},
   "source": [
    "### Test GPT2 with 1% recipe input\n",
    "Is the input ready to be used by GPT2 type model? Test with pretrained model with fine tuning for working example then work on model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748aa36-213f-4e88-9d5a-3a92c2ed4002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load your formatted text dataset with datasets library\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434aa60edf6845d09b3436510188adbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133866 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data collator for language modeling (no masked language model here)\n",
      "Define training arguments\n",
      "Setup the Trainer\n",
      "Start fine-tuning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1603' max='200799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1603/200799 08:45 < 18:10:12, 3.05 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.975300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.715500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.730400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.782700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.771600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load tokenizer & model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"Load your formatted text dataset with datasets library\")\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"./RecipeNLG/RecipeNLG_dataset_gpt_in_1pct.txt\"})\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=512, padding=\"max_length\")  # Ensure padding\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(\"Prepare data collator for language modeling (no masked language model here)\")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # GPT uses causal LM, not masked LM like BERT\n",
    ")\n",
    "\n",
    "print(\"Define training arguments\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-recipes\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "print(\"Setup the Trainer\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "print(\"Start fine-tuning\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Save the final model and tokenizer\")\n",
    "model.save_pretrained(\"./gpt2-recipes-final\")\n",
    "tokenizer.save_pretrained(\"./gpt2-recipes-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc2416-1c1f-4ef6-84cd-da0d2d18970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876f439-220a-48fb-b92a-e2eb8f4eaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349eb75-098f-46d3-94aa-310d2abbd4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
